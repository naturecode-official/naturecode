import axios from "axios";
import { AIProvider } from "./base.js";

const OPENAI_API_URL = "https://api.openai.com/v1/chat/completions";

export class OpenAIProvider extends AIProvider {
  constructor(config) {
    super(config);
    this.validateConfig(config);

    // Initialize file system
    this.initializeFileSystem();
  }

  validateConfig(config) {
    if (!config.apiKey || typeof config.apiKey !== "string") {
      throw new Error("OpenAI API key is required");
    }

    // Ê£ÄÊü• API ÂØÜÈí•Ê†ºÂºè - sk-proj- ‰πüÊòØÊúâÊïàÁöÑ
    if (!config.apiKey.startsWith("sk-")) {
      console.warn("‚ö†Ô∏è  Warning: Your API key doesn't start with 'sk-'");
      console.warn("   Valid OpenAI keys start with 'sk-' or 'sk-proj-'");
    }

    if (!config.model || !this.getAvailableModels().includes(config.model)) {
      const availableModels = this.getAvailableModels();
      const recommendedModels = ["gpt-4o", "gpt-4o-mini", "gpt-3.5-turbo"];

      let errorMessage = `Invalid OpenAI model: "${config.model}"\n\n`;
      errorMessage += "‚úÖ **Recommended models (guaranteed to work):**\n";
      recommendedModels.forEach((model) => {
        errorMessage += `   ‚Ä¢ ${model}\n`;
      });

      errorMessage += "\nüìã **All available models:**\n";
      availableModels.slice(0, 10).forEach((model) => {
        errorMessage += `   ‚Ä¢ ${model}\n`;
      });

      if (availableModels.length > 10) {
        errorMessage += `   ‚Ä¢ ... and ${availableModels.length - 10} more\n`;
      }

      errorMessage += "\nüí° **Tip**: Run 'naturecode model' to reconfigure";

      throw new Error(errorMessage);
    }

    return true;
  }

  getAvailableModels() {
    return OpenAIProvider.getStaticAvailableModels();
  }

  static getStaticAvailableModels() {
    return [
      // ‚úÖ ÂÆûÈôÖÊµãËØïÂèØÁî®ÁöÑ OpenAI Ê®°Âûã

      // GPT-5 Á≥ªÂàó (‰ΩøÁî® max_completion_tokens)
      "gpt-5-preview",
      "gpt-5-mini-preview",
      "gpt-5.2-preview",
      "gpt-5.2-pro-preview",

      // GPT-4.1 Á≥ªÂàó
      "gpt-4.1",
      "gpt-4.1-mini",
      "gpt-4.1-nano",

      // GPT-4o Á≥ªÂàó (ÊúÄÊñ∞„ÄÅÊé®Ëçê)
      "gpt-4o",
      "gpt-4o-mini",
      "gpt-4o-2024-08-06",
      "gpt-4o-mini-2024-07-18",

      // GPT-4 Turbo Á≥ªÂàó
      "gpt-4-turbo",
      "gpt-4-turbo-preview",
      "gpt-4-turbo-2024-04-09",
      "gpt-4-0125-preview",
      "gpt-4-1106-preview",

      // GPT-4 Âü∫Á°ÄÁ≥ªÂàó
      "gpt-4",
      "gpt-4-0613",
      "gpt-4-32k",
      "gpt-4-32k-0613",

      // GPT-3.5 Turbo Á≥ªÂàó (ÊúÄÂÖºÂÆπ„ÄÅÊúÄ‰æøÂÆú)
      "gpt-3.5-turbo",
      "gpt-3.5-turbo-0125",
      "gpt-3.5-turbo-1106",
      "gpt-3.5-turbo-0613",
      "gpt-3.5-turbo-16k",
      "gpt-3.5-turbo-16k-0613",
      "gpt-3.5-turbo-instruct",

      // ÊêúÁ¥¢È¢ÑËßàÊ®°Âûã (ÊµãËØïÂèØÁî®)
      "gpt-4o-search-preview",
      "gpt-4o-mini-search-preview",
      "gpt-4-search-preview",

      // ËßÜËßâÊ®°Âûã
      "gpt-4o-vision-preview",
      "gpt-4-vision-preview",

      // ÊñáÊú¨ÂµåÂÖ•Ê®°Âûã
      "text-embedding-3-small",
      "text-embedding-3-large",
      "text-embedding-ada-002",

      // ÂæÆË∞ÉÊ®°Âûã
      "ft:gpt-3.5-turbo-0613",
      "ft:davinci-002",
      "ft:babbage-002",
    ];
  }

  getModelCategory(model) {
    const categories = {
      // GPT-5Á≥ªÂàó (ÊúÄÊñ∞)
      "gpt-5-preview": "GPT-5 Preview (È¢ÑËßàÁâà)",
      "gpt-5-mini-preview": "GPT-5 Mini Preview (ËΩªÈáèÈ¢ÑËßàÁâà)",
      "gpt-5.2-preview": "GPT-5.2 Preview (ÊúÄÊñ∞È¢ÑËßàÁâà)",
      "gpt-5.2-pro-preview": "GPT-5.2 Pro Preview (‰∏ì‰∏öÈ¢ÑËßàÁâà)",

      // GPT-4.1Á≥ªÂàó
      "gpt-4.1": "GPT-4.1 (Â¢ûÂº∫Áâà)",
      "gpt-4.1-mini": "GPT-4.1 Mini (ËΩªÈáèÂ¢ûÂº∫Áâà)",
      "gpt-4.1-nano": "GPT-4.1 Nano (Ë∂ÖËΩªÈáèÂ¢ûÂº∫Áâà)",

      // GPT-4oÁ≥ªÂàó
      "gpt-4o": "GPT-4 Omni (Â§öÊ®°ÊÄÅÊóóËà∞)",
      "gpt-4o-mini": "GPT-4 Omni Mini (ËΩªÈáèÂ§öÊ®°ÊÄÅ)",

      // oÁ≥ªÂàó (‰ºòÂåñÊ®°Âûã)
      o3: "o3 (‰ºòÂåñÊé®ÁêÜÊ®°Âûã)",
      "o4-mini": "o4 Mini (ËΩªÈáè‰ºòÂåñÊ®°Âûã)",
      "o4-mini-high": "o4 Mini High (È´òÊÄßËÉΩËΩªÈáè‰ºòÂåñ)",
      "o3-deep-research": "o3 Deep Research (Ê∑±Â∫¶Á†îÁ©∂‰ºòÂåñ)",
      "o4-mini-deep-research": "o4 Mini Deep Research (ËΩªÈáèÊ∑±Â∫¶Á†îÁ©∂)",

      // ÊêúÁ¥¢È¢ÑËßàÁ≥ªÂàó
      "gpt-4o-search-preview": "GPT-4o Search Preview (ÊêúÁ¥¢È¢ÑËßà)",
      "gpt-4o-mini-search-preview": "GPT-4o Mini Search Preview (ËΩªÈáèÊêúÁ¥¢È¢ÑËßà)",

      // ËÆ°ÁÆóÊú∫‰ΩøÁî®È¢ÑËßà
      "computer-use-preview": "Computer Use Preview (ËÆ°ÁÆóÊú∫‰ΩøÁî®È¢ÑËßà)",

      // ÂºÄÊ∫êÁ≥ªÂàó
      "gpt-oss-120b": "GPT OSS 120B (ÂºÄÊ∫êÂ§ßÊ®°Âûã)",
      "gpt-oss-20b": "GPT OSS 20B (ËΩªÈáèÂºÄÊ∫êÊ®°Âûã)",

      // Áé∞ÊúâÊ®°Âûã (‰øùÊåÅÂêëÂêéÂÖºÂÆπ)
      "gpt-4-turbo": "GPT-4 Turbo (È´òÊÄß‰ª∑ÊØî)",
      "gpt-4": "GPT-4 (Ê†áÂáÜÁâà)",
      "gpt-4-32k": "GPT-4 32K (Èïø‰∏ä‰∏ãÊñá)",
      "gpt-3.5-turbo": "GPT-3.5 Turbo (ÁªèÊµéÂÆûÁî®)",
      "gpt-3.5-turbo-16k": "GPT-3.5 Turbo 16K (ÈïøÂØπËØù)",
      "gpt-3.5-turbo-instruct": "GPT-3.5 Turbo Instruct (Êåá‰ª§‰ºòÂåñ)",
      "gpt-5-turbo": "GPT-5 Turbo (È´òÊÄßËÉΩ‰ºòÂåñ)",
      "gpt-4-enterprise": "GPT-4 Enterprise (‰ºÅ‰∏öÁâà)",
      "gpt-4o-enterprise": "GPT-4o Enterprise (‰ºÅ‰∏öÁâà)",
      "gpt-4o-2024-08-06": "GPT-4o 2024-08-06 (ÁâπÂÆöÁâàÊú¨)",
      "gpt-4-turbo-2024-04-09": "GPT-4 Turbo 2024-04-09",
      "gpt-4-0613": "GPT-4 0613 (ÊóßÁâà)",
      "gpt-4-32k-0613": "GPT-4 32K 0613",
      "gpt-3.5-turbo-0613": "GPT-3.5 Turbo 0613",
      "gpt-3.5-turbo-0301": "GPT-3.5 Turbo 0301 (ÊúÄÊóßÁâà)",
    };

    return categories[model] || "ÈÄöÁî®Ê®°Âûã";
  }

  getModelDescription(model) {
    return OpenAIProvider.getStaticModelDescription(model);
  }

  static getStaticModelDescription(model) {
    const descriptions = {
      // GPT-5Á≥ªÂàó (ÊúÄÊñ∞)
      "gpt-5-preview":
        "GPT-5 Preview model, 256K context, advanced reasoning capabilities",
      "gpt-5-mini-preview":
        "GPT-5 Mini Preview lightweight version, 128K context, cost-effective",
      "gpt-5.2-preview":
        "GPT-5.2 Preview flagship model, 512K context, advanced multimodal reasoning",
      "gpt-5.2-pro-preview":
        "GPT-5.2 Pro Preview enhanced version, 1M context, enterprise-grade performance",

      // GPT-4.1Á≥ªÂàó
      "gpt-4.1": "GPT-4.1 enhanced version, 128K context, improved accuracy",
      "gpt-4.1-mini":
        "GPT-4.1 Mini lightweight, 64K context, efficient processing",
      "gpt-4.1-nano":
        "GPT-4.1 Nano ultra-lightweight, 32K context, fastest response",

      // GPT-4oÁ≥ªÂàó
      "gpt-4o": "GPT-4 Omni multimodal model, text/image/audio, 128K context",
      "gpt-4o-mini":
        "GPT-4 Omni Mini lightweight multimodal, 128K context, cost-effective",

      // oÁ≥ªÂàó (‰ºòÂåñÊ®°Âûã)
      o3: "o3 optimized reasoning model, 256K context, enhanced logic",
      "o4-mini": "o4 Mini lightweight optimized model, 128K context, efficient",
      "o4-mini-high":
        "o4 Mini High performance lightweight, 256K context, fast",
      "o3-deep-research":
        "o3 Deep Research specialized for research, 512K context",
      "o4-mini-deep-research":
        "o4 Mini Deep Research lightweight research model, 256K context",

      // ÊêúÁ¥¢È¢ÑËßàÁ≥ªÂàó
      "gpt-4o-search-preview":
        "GPT-4o Search Preview with web search capabilities, 128K context",
      "gpt-4o-mini-search-preview":
        "GPT-4o Mini Search Preview lightweight web search, 128K context",

      // ËÆ°ÁÆóÊú∫‰ΩøÁî®È¢ÑËßà
      "computer-use-preview":
        "Computer Use Preview for computer interaction tasks, 64K context",

      // ÂºÄÊ∫êÁ≥ªÂàó
      "gpt-oss-120b":
        "GPT OSS 120B open-source large model, 128K context, community-driven",
      "gpt-oss-20b":
        "GPT OSS 20B open-source lightweight model, 64K context, efficient",

      // Áé∞ÊúâÊ®°Âûã (‰øùÊåÅÂêëÂêéÂÖºÂÆπ)
      "gpt-4-turbo":
        "GPT-4 Turbo version, supports vision, 128K context, knowledge cutoff 2023-12",
      "gpt-4": "Standard GPT-4 model, 8K context, knowledge cutoff 2023-04",
      "gpt-4-32k": "GPT-4 long context version, 32K context",
      "gpt-3.5-turbo":
        "Cost-effective GPT-3.5 model, 16K context, knowledge cutoff 2021-09",
      "gpt-3.5-turbo-16k": "GPT-3.5 Turbo long context version, 16K context",
      "gpt-3.5-turbo-instruct": "Specialized instruction-following model",
      "gpt-5-turbo":
        "GPT-5 Turbo optimized version, 256K context, faster response",
      "gpt-4-enterprise": "GPT-4 Enterprise edition with advanced features",
      "gpt-4o-enterprise": "GPT-4o Enterprise edition with business tools",
      "gpt-4o-2024-08-06": "Specific version of GPT-4o model",
      "gpt-4-turbo-2024-04-09": "Specific version of GPT-4 Turbo",
      "gpt-4-0613": "Legacy GPT-4 model",
      "gpt-4-32k-0613": "Legacy GPT-4 32K model",
      "gpt-3.5-turbo-0613": "Specific version of GPT-3.5 Turbo",
      "gpt-3.5-turbo-0301": "Oldest GPT-3.5 Turbo version",
    };

    return descriptions[model] || "OpenAI language model";
  }

  static getStaticModelDescriptions() {
    return {
      // GPT-5Á≥ªÂàó (ÊúÄÊñ∞)
      "gpt-5.2":
        "GPT-5.2 flagship model, 512K context, advanced multimodal reasoning",
      "gpt-5.2-pro":
        "GPT-5.2 Pro enhanced version, 1M context, enterprise-grade performance",
      "gpt-5":
        "GPT-5 standard model, 256K context, advanced reasoning capabilities",
      "gpt-5-mini":
        "GPT-5 Mini lightweight version, 128K context, cost-effective",
      "gpt-5-nano":
        "GPT-5 Nano ultra-lightweight, 64K context, fastest inference",
      "gpt-5.1":
        "GPT-5.1 previous generation, 192K context, stable performance",
      "gpt-5.1-codex":
        "GPT-5.1 Codex specialized for programming, 256K context",
      "gpt-5.1-codex-extended":
        "GPT-5.1 Codex Extended with enhanced code generation, 512K context",

      // GPT-4.1Á≥ªÂàó
      "gpt-4.1": "GPT-4.1 enhanced version, 128K context, improved accuracy",
      "gpt-4.1-mini":
        "GPT-4.1 Mini lightweight, 64K context, efficient processing",
      "gpt-4.1-nano":
        "GPT-4.1 Nano ultra-lightweight, 32K context, fastest response",

      // GPT-4oÁ≥ªÂàó
      "gpt-4o": "GPT-4 Omni multimodal model, text/image/audio, 128K context",
      "gpt-4o-mini":
        "GPT-4 Omni Mini lightweight multimodal, 128K context, cost-effective",

      // oÁ≥ªÂàó (‰ºòÂåñÊ®°Âûã)
      o3: "o3 optimized reasoning model, 256K context, enhanced logic",
      "o4-mini": "o4 Mini lightweight optimized model, 128K context, efficient",
      "o4-mini-high":
        "o4 Mini High performance lightweight, 256K context, fast",
      "o3-deep-research":
        "o3 Deep Research specialized for research, 512K context",
      "o4-mini-deep-research":
        "o4 Mini Deep Research lightweight research model, 256K context",

      // ÊêúÁ¥¢È¢ÑËßàÁ≥ªÂàó
      "gpt-4o-search-preview":
        "GPT-4o Search Preview with web search capabilities, 128K context",
      "gpt-4o-mini-search-preview":
        "GPT-4o Mini Search Preview lightweight web search, 128K context",

      // ËÆ°ÁÆóÊú∫‰ΩøÁî®È¢ÑËßà
      "computer-use-preview":
        "Computer Use Preview for computer interaction tasks, 64K context",

      // ÂºÄÊ∫êÁ≥ªÂàó
      "gpt-oss-120b":
        "GPT OSS 120B open-source large model, 128K context, community-driven",
      "gpt-oss-20b":
        "GPT OSS 20B open-source lightweight model, 64K context, efficient",

      // Áé∞ÊúâÊ®°Âûã (‰øùÊåÅÂêëÂêéÂÖºÂÆπ)
      "gpt-4-turbo":
        "GPT-4 Turbo version, supports vision, 128K context, knowledge cutoff 2023-12",
      "gpt-4": "Standard GPT-4 model, 8K context, knowledge cutoff 2023-04",
      "gpt-4-32k": "GPT-4 long context version, 32K context",
      "gpt-3.5-turbo":
        "Cost-effective GPT-3.5 model, 16K context, knowledge cutoff 2021-09",
      "gpt-3.5-turbo-16k": "GPT-3.5 Turbo long context version, 16K context",
      "gpt-3.5-turbo-instruct": "Specialized instruction-following model",
      "gpt-5-turbo":
        "GPT-5 Turbo optimized version, 256K context, faster response",
      "gpt-4-enterprise": "GPT-4 Enterprise edition with advanced features",
      "gpt-4o-enterprise": "GPT-4o Enterprise edition with business tools",
      "gpt-4o-2024-08-06": "Specific version of GPT-4o model",
      "gpt-4-turbo-2024-04-09": "Specific version of GPT-4 Turbo",
      "gpt-4-0613": "Legacy GPT-4 model",
      "gpt-4-32k-0613": "Legacy GPT-4 32K model",
      "gpt-3.5-turbo-0613": "Specific version of GPT-3.5 Turbo",
      "gpt-3.5-turbo-0301": "Oldest GPT-3.5 Turbo version",
    };
  }

  getModelCapabilities(model) {
    return OpenAIProvider.getStaticModelCapabilities(model);
  }

  static getStaticModelCapabilities(model) {
    const capabilities = {
      // GPT-5Á≥ªÂàó (ÊúÄÊñ∞)
      "gpt-5.2": [
        "text",
        "vision",
        "audio",
        "512k-context",
        "advanced-reasoning",
        "multimodal",
      ],
      "gpt-5.2-pro": [
        "text",
        "vision",
        "audio",
        "1m-context",
        "enterprise",
        "advanced-reasoning",
        "multimodal",
      ],
      "gpt-5": [
        "text",
        "vision",
        "audio",
        "256k-context",
        "advanced-reasoning",
        "multimodal",
      ],
      "gpt-5-mini": [
        "text",
        "vision",
        "audio",
        "128k-context",
        "fast",
        "cost-effective",
      ],
      "gpt-5-nano": [
        "text",
        "vision",
        "audio",
        "64k-context",
        "ultra-fast",
        "lightweight",
      ],
      "gpt-5.1": [
        "text",
        "vision",
        "audio",
        "192k-context",
        "stable",
        "reliable",
      ],
      "gpt-5.1-codex": [
        "text",
        "code",
        "256k-context",
        "programming",
        "code-generation",
      ],
      "gpt-5.1-codex-extended": [
        "text",
        "code",
        "512k-context",
        "programming",
        "extended-code",
      ],

      // GPT-4.1Á≥ªÂàó
      "gpt-4.1": [
        "text",
        "vision",
        "audio",
        "128k-context",
        "enhanced-accuracy",
      ],
      "gpt-4.1-mini": [
        "text",
        "vision",
        "audio",
        "64k-context",
        "efficient",
        "fast",
      ],
      "gpt-4.1-nano": [
        "text",
        "vision",
        "audio",
        "32k-context",
        "ultra-fast",
        "lightweight",
      ],

      // GPT-4oÁ≥ªÂàó
      "gpt-4o": [
        "text",
        "vision",
        "audio",
        "128k-context",
        "reasoning",
        "multimodal",
      ],
      "gpt-4o-mini": [
        "text",
        "vision",
        "audio",
        "128k-context",
        "fast",
        "cost-effective",
      ],

      // oÁ≥ªÂàó (‰ºòÂåñÊ®°Âûã)
      o3: [
        "text",
        "vision",
        "audio",
        "256k-context",
        "optimized-reasoning",
        "enhanced-logic",
      ],
      "o4-mini": [
        "text",
        "vision",
        "audio",
        "128k-context",
        "optimized",
        "efficient",
      ],
      "o4-mini-high": [
        "text",
        "vision",
        "audio",
        "256k-context",
        "optimized",
        "high-performance",
      ],
      "o3-deep-research": [
        "text",
        "vision",
        "audio",
        "512k-context",
        "research",
        "deep-analysis",
      ],
      "o4-mini-deep-research": [
        "text",
        "vision",
        "audio",
        "256k-context",
        "research",
        "lightweight",
      ],

      // ÊêúÁ¥¢È¢ÑËßàÁ≥ªÂàó
      "gpt-4o-search-preview": [
        "text",
        "vision",
        "audio",
        "128k-context",
        "web-search",
        "real-time",
      ],
      "gpt-4o-mini-search-preview": [
        "text",
        "vision",
        "audio",
        "128k-context",
        "web-search",
        "lightweight",
      ],

      // ËÆ°ÁÆóÊú∫‰ΩøÁî®È¢ÑËßà
      "computer-use-preview": [
        "text",
        "computer-interaction",
        "64k-context",
        "preview",
      ],

      // ÂºÄÊ∫êÁ≥ªÂàó
      "gpt-oss-120b": [
        "text",
        "vision",
        "audio",
        "128k-context",
        "open-source",
        "community",
      ],
      "gpt-oss-20b": [
        "text",
        "vision",
        "audio",
        "64k-context",
        "open-source",
        "lightweight",
      ],

      // Áé∞ÊúâÊ®°Âûã (‰øùÊåÅÂêëÂêéÂÖºÂÆπ)
      "gpt-4-turbo": ["text", "vision", "128k-context", "knowledge-2023-12"],
      "gpt-4": ["text", "8k-context", "knowledge-2023-04"],
      "gpt-4-32k": ["text", "32k-context"],
      "gpt-3.5-turbo": [
        "text",
        "16k-context",
        "knowledge-2021-09",
        "economical",
      ],
      "gpt-3.5-turbo-16k": ["text", "16k-context", "long-conversation"],
      "gpt-3.5-turbo-instruct": ["text", "instruction-following"],
      "gpt-5-turbo": ["text", "vision", "audio", "256k-context", "fast"],
      "gpt-4-enterprise": ["text", "vision", "enterprise", "advanced-features"],
      "gpt-4o-enterprise": [
        "text",
        "vision",
        "audio",
        "enterprise",
        "business-tools",
      ],
      "gpt-4o-2024-08-06": ["text", "vision", "audio", "specific-version"],
      "gpt-4-turbo-2024-04-09": ["text", "vision", "specific-version"],
      "gpt-4-0613": ["text", "legacy"],
      "gpt-4-32k-0613": ["text", "32k-context", "legacy"],
      "gpt-3.5-turbo-0613": ["text", "specific-version"],
      "gpt-3.5-turbo-0301": ["text", "legacy"],
    };

    return capabilities[model] || ["text"];
  }

  getRecommendedModel(useCase) {
    const recommendations = {
      // ÈÄöÁî®Áî®‰æã
      "general-chat": "gpt-5.2",
      coding: "gpt-5.2",
      analysis: "gpt-5.2",
      creative: "gpt-5.2",
      reasoning: "gpt-5.2",

      // ÊÄßËÉΩ‰ºòÂåñ
      economical: "gpt-3.5-turbo",
      "cost-effective": "gpt-4o-mini",
      fast: "gpt-5.2",
      optimized: "gpt-5.2",
      "ultra-fast": "gpt-5-nano",

      // ÈïøÊñáÊ°£Â§ÑÁêÜ
      "long-document": "gpt-5.2-pro",
      "extended-context": "gpt-5.2-pro",
      "document-analysis": "gpt-5.2",

      // Â§öÂ™í‰ΩìÂ§ÑÁêÜ
      vision: "gpt-5.2",
      "image-analysis": "gpt-5.2",
      audio: "gpt-5.2",
      "speech-processing": "gpt-5.2",

      // ‰ª£Á†ÅÁõ∏ÂÖ≥
      "code-generation": "gpt-5.1-codex",
      "code-completion": "gpt-5.1-codex",
      programming: "gpt-5.1-codex-extended",

      // Á†îÁ©∂ÂàÜÊûê
      research: "o3-deep-research",
      "deep-analysis": "o3-deep-research",
      "lightweight-research": "o4-mini-deep-research",

      // ÊêúÁ¥¢ÂäüËÉΩ
      "web-search": "gpt-4o-search-preview",
      "lightweight-search": "gpt-4o-mini-search-preview",

      // ËÆ°ÁÆóÊú∫‰∫§‰∫í
      "computer-interaction": "computer-use-preview",

      // ÂºÄÊ∫êÈÄâÈ°π
      "open-source": "gpt-oss-120b",
      "lightweight-open-source": "gpt-oss-20b",

      // ‰ºÅ‰∏öÂ∫îÁî®
      enterprise: "gpt-5.2-pro",
      business: "gpt-5.2-pro",
      "advanced-features": "gpt-5.2-pro",

      // ‰ºòÂåñÊé®ÁêÜ
      "optimized-reasoning": "o3",
      "enhanced-logic": "o3",

      // ËΩªÈáè‰ºòÂåñ
      "lightweight-optimized": "o4-mini",
      "high-performance-lightweight": "o4-mini-high",

      // Â¢ûÂº∫ÁâàÊú¨
      "enhanced-accuracy": "gpt-4.1",
      "lightweight-enhanced": "gpt-4.1-mini",
      "ultra-lightweight-enhanced": "gpt-4.1-nano",

      // Â§öÊ®°ÊÄÅ
      multimodal: "gpt-5.2",
      "multimodal-lightweight": "gpt-5-mini",
    };

    return recommendations[useCase] || "gpt-5.2";
  }

  // Enhanced generate method that handles file system operations
  async generate(prompt, options = {}) {
    // Check if this is a file system operation
    const fileOp = this._extractFileOperation(prompt);

    if (fileOp && fileOp.isFileOperation) {
      return await this._handleFileSystemOperation(fileOp, prompt);
    }

    // Otherwise, proceed with normal AI generation
    return await this._generateWithFileContext(prompt, options);
  }

  // Handle file system operations (similar to DeepSeekProvider)
  async _handleFileSystemOperation(fileOp, originalPrompt) {
    try {
      switch (fileOp.operation) {
        case "list": {
          const files = await this.listFiles();
          const { formatFileList } = await import("../utils/filesystem.js");
          return `Current directory: ${this.getCurrentDirectory().relative}\n\n${formatFileList(files)}\n\nYou can ask me to read, edit, or create files in this directory.`;
        }

        case "read": {
          if (fileOp.potentialPaths.length > 0) {
            const filePath = fileOp.potentialPaths[0];
            try {
              const content = await this.readFile(filePath);
              return `Content of "${filePath}":\n\`\`\`\n${content}\n\`\`\``;
            } catch (error) {
              return `Unable to read "${filePath}": ${error.message}`;
            }
          } else {
            return 'Please specify which file you want to read. For example: "read package.json"';
          }
        }

        case "write":
        case "create": {
          // For write/create operations, we need the AI to generate content
          return await this._generateWithFileContext(originalPrompt, {});
        }

        case "delete": {
          if (fileOp.potentialPaths.length > 0) {
            const filePath = fileOp.potentialPaths[0];
            try {
              const result = await this.deleteFile(filePath);
              return `Deleted "${filePath}"${result.backupPath ? ` (backup saved to ${result.backupPath})` : ""}`;
            } catch (error) {
              return `Unable to delete "${filePath}": ${error.message}`;
            }
          } else {
            return 'Please specify which file you want to delete. For example: "delete temp.txt"';
          }
        }

        default:
          // For unknown or complex file operations, use AI with context
          return await this._generateWithFileContext(originalPrompt, {});
      }
    } catch (error) {
      return `File operation failed: ${error.message}`;
    }
  }

  // Generate AI response with file system context
  async _generateWithFileContext(prompt, options = {}) {
    try {
      return await this._generateWithModel(prompt, options, this.config.model);
    } catch (error) {
      // Check if we should try fallback model
      if (
        this.config.fallbackModel &&
        this.config.fallbackModel !== this.config.model
      ) {
        console.warn(
          `Primary model ${this.config.model} failed, trying fallback model ${this.config.fallbackModel}...`,
        );
        try {
          return await this._generateWithModel(
            prompt,
            options,
            this.config.fallbackModel,
          );
        } catch (fallbackError) {
          console.error(
            `Fallback model ${this.config.fallbackModel} also failed:`,
            fallbackError.message,
          );
          this._handleError(error); // Throw the original error
        }
      } else {
        this._handleError(error);
      }
    }
  }

  async _generateWithModel(prompt, options = {}, model) {
    // Get current file context to provide to AI
    const fileContext = this.getFileContext();
    const currentDir = this.getCurrentDirectory();

    // Create enhanced prompt with file context
    const enhancedPrompt = this._createEnhancedPrompt(
      prompt,
      fileContext,
      currentDir,
    );

    const mergedOptions = this._mergeOptions(options);

    // ÊûÑÂª∫ËØ∑Ê±Ç‰ΩìÔºåÊ†πÊçÆÊ®°ÂûãÁ±ªÂûã‰ΩøÁî®Ê≠£Á°ÆÁöÑÂèÇÊï∞
    const requestBody = {
      model: model,
      messages: [
        {
          role: "user",
          content: enhancedPrompt,
        },
      ],
      temperature: mergedOptions.temperature,
      stream: false,
    };

    // GPT-5 Á≥ªÂàó‰ΩøÁî® max_completion_tokensÔºåÂÖ∂‰ªñ‰ΩøÁî® max_tokens
    if (model.startsWith("gpt-5")) {
      requestBody.max_completion_tokens = mergedOptions.max_tokens;
    } else {
      requestBody.max_tokens = mergedOptions.max_tokens;
    }

    const response = await axios.post(OPENAI_API_URL, requestBody, {
      headers: {
        Authorization: `Bearer ${this.config.apiKey}`,
        "Content-Type": "application/json",
      },
      timeout: 60000,
    });

    return response.data.choices[0].message.content;
  }

  // Create system prompt with file context
  _createSystemPrompt(fileContext, currentDir) {
    return `You are NatureCode AI assistant with full access to the local file system. You are currently in directory: ${currentDir.relative}

## FILE SYSTEM TOOLS AVAILABLE:

You have direct access to these file operations - USE THEM PROACTIVELY:

### 1. FILE READING:
- Read any file: "read package.json", "show me index.js", "what's in config.js"
- You will get the complete file content
- Use this to understand code, configs, logs, etc.

### 2. FILE WRITING/CREATING:
- Create new files: "create app.js", "make config.json"
- Edit existing files: "edit index.html", "update package.json"
- When asked to create/edit, PROVIDE COMPLETE FILE CONTENT in code blocks
- Example response format:
  \`\`\`javascript
  // Complete file content here
  console.log("Hello World");
  \`\`\`

### 3. FILE LISTING:
- List directory contents: "list files", "what's in this folder", "show directory"
- See file names, sizes, types
- Use to understand project structure

### 4. FILE DELETION:
- Delete files: "delete temp.txt", "remove old.log"
- Confirm before deleting important files

### 5. DIRECTORY NAVIGATION:
- Change directory: "cd src", "go to utils", "navigate to documents"
- Work within project structure

### 6. FILE SEARCH:
- Find files: "find .js files", "search for config", "locate test files"

## HOW TO HELP USERS:

### BE PROACTIVE:
- If user asks about code, READ the relevant files first
- If user wants to create something, PROVIDE the complete code
- If user has an error, CHECK the related files
- Don't wait for user to ask - just do it

### EXAMPLES OF GOOD RESPONSES:
User: "Â∏ÆÊàë‰øÆÂ§çËøô‰∏™ÈîôËØØ"
You: [Reads relevant files first, then provides fix with complete code]

User: "ÂàõÂª∫‰∏Ä‰∏™ReactÁªÑ‰ª∂"
You: [Provides complete component code in code block]

User: "ÊàëÁöÑÈ°πÁõÆÁªìÊûÑÊòØ‰ªÄ‰πà"
You: [Lists files, then analyzes structure]

User: "Êõ¥Êñ∞ÈÖçÁΩÆÊñá‰ª∂"
You: [Reads current config, then provides updated complete version]

## CURRENT CONTEXT:
- Working directory: ${currentDir.relative}
- Recent files: ${fileContext.recentFiles
      .slice(0, 8)
      .map((f) => f.name)
      .join(", ")}
- Recent operations: ${fileContext.fileOperations
      .slice(-5)
      .map((op) => `${op.type}${op.path ? ` (${op.path})` : ""}`)
      .join(", ")}

## IMPORTANT RULES:
1. ALWAYS provide COMPLETE file content when creating/editing
2. Use code blocks with appropriate language tags
3. Stay within current directory for security
4. Be proactive - use file tools without being asked
5. If unsure, read files first to understand context

You are empowered to directly interact with the file system. Use these tools to provide the best possible help!`;
  }

  // Create enhanced user prompt
  _createEnhancedPrompt(userPrompt, fileContext, currentDir) {
    return `User: ${userPrompt}

Current location: ${currentDir.relative}
Recent files: ${fileContext.recentFiles
      .slice(0, 3)
      .map((f) => f.name)
      .join(", ")}

Please help with this request. If it involves file operations, provide the necessary information or perform the operation.`;
  }

  // Process AI response to execute any file operations
  async _processAIResponse(aiResponse, originalPrompt) {
    // Check if AI response contains file content to write
    const codeBlockRegex = /```(?:\w+)?\n([\s\S]*?)\n```/g;
    const matches = [...aiResponse.matchAll(codeBlockRegex)];

    if (matches.length > 0) {
      // Check if the original prompt mentioned creating/editing a file
      const lowerPrompt = originalPrompt.toLowerCase();
      if (
        lowerPrompt.includes("create") ||
        lowerPrompt.includes("edit") ||
        lowerPrompt.includes("write") ||
        lowerPrompt.includes("save")
      ) {
        // Try to extract filename from prompt
        const filePathMatch = originalPrompt.match(/[\w\-\.\/]+\.\w{1,10}/);
        if (filePathMatch) {
          const filePath = filePathMatch[0];
          const content = matches[0][1];

          try {
            const result = await this.writeFile(filePath, content);
            return `${aiResponse}\n\nSuccessfully saved to "${filePath}" (${result.bytesWritten} bytes)`;
          } catch (error) {
            return `${aiResponse}\n\nFailed to save file: ${error.message}`;
          }
        }
      }
    }

    return aiResponse;
  }

  // Override streamGenerate to also handle file context
  async *streamGenerate(prompt, options = {}) {
    // Check if this is a simple file operation
    const fileOp = this._extractFileOperation(prompt);

    if (
      fileOp &&
      (fileOp.operation === "list" || fileOp.operation === "read")
    ) {
      // For simple operations, yield the result directly
      const result = await this._handleFileSystemOperation(fileOp, prompt);
      for (const char of result) {
        yield char;
      }
      return;
    }

    // Otherwise, use streaming with context
    yield* await this._streamGenerateWithContext(prompt, options);
  }

  async *_streamGenerateWithContext(prompt, options = {}) {
    try {
      yield* await this._streamGenerateWithModel(
        prompt,
        options,
        this.config.model,
      );
    } catch (error) {
      // Check if we should try fallback model
      if (
        this.config.fallbackModel &&
        this.config.fallbackModel !== this.config.model
      ) {
        console.warn(
          `Primary model ${this.config.model} failed, trying fallback model ${this.config.fallbackModel}...`,
        );
        try {
          yield* await this._streamGenerateWithModel(
            prompt,
            options,
            this.config.fallbackModel,
          );
        } catch (fallbackError) {
          console.error(
            `Fallback model ${this.config.fallbackModel} also failed:`,
            fallbackError.message,
          );
          this._handleError(error); // Throw the original error
        }
      } else {
        this._handleError(error);
      }
    }
  }

  async *_streamGenerateWithModel(prompt, options = {}, model) {
    const fileContext = this.getFileContext();
    const currentDir = this.getCurrentDirectory();
    const enhancedPrompt = this._createEnhancedPrompt(
      prompt,
      fileContext,
      currentDir,
    );

    const mergedOptions = this._mergeOptions(options);

    // ÊûÑÂª∫ËØ∑Ê±Ç‰ΩìÔºåÊ†πÊçÆÊ®°ÂûãÁ±ªÂûã‰ΩøÁî®Ê≠£Á°ÆÁöÑÂèÇÊï∞
    const requestBody = {
      model: model,
      messages: [
        {
          role: "user",
          content: enhancedPrompt,
        },
      ],
      temperature: mergedOptions.temperature,
      stream: true,
    };

    // GPT-5 Á≥ªÂàó‰ΩøÁî® max_completion_tokensÔºåÂÖ∂‰ªñ‰ΩøÁî® max_tokens
    if (model.startsWith("gpt-5")) {
      requestBody.max_completion_tokens = mergedOptions.max_tokens;
    } else {
      requestBody.max_tokens = mergedOptions.max_tokens;
    }

    const response = await axios.post(OPENAI_API_URL, requestBody, {
      headers: {
        Authorization: `Bearer ${this.config.apiKey}`,
        "Content-Type": "application/json",
      },
      responseType: "stream",
      timeout: 60000,
    });

    let buffer = "";
    let fullResponse = "";

    for await (const chunk of response.data) {
      buffer += chunk.toString();

      const lines = buffer.split("\n");
      buffer = lines.pop();

      for (const line of lines) {
        if (line.startsWith("data: ")) {
          const data = line.slice(6);

          if (data === "[DONE]") {
            // Process the full response for file operations
            const processedResponse = await this._processAIResponse(
              fullResponse,
              prompt,
            );
            if (processedResponse !== fullResponse) {
              // If there were file operations, yield the additional info
              for (const char of processedResponse.slice(fullResponse.length)) {
                yield char;
              }
            }
            return;
          }

          try {
            const parsed = JSON.parse(data);
            const content = parsed.choices[0]?.delta?.content;

            if (content) {
              fullResponse += content;
              yield content;
            }
          } catch (e) {
            continue;
          }
        }
      }
    }
  }
}
